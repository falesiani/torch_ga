{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch_ga import GeometricAlgebra\n",
    "# from torch_ga.blades import BladeKind\n",
    "# from torch_ga.layers import GeometricProductConv1D\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions import transforms as tT\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "\n",
    "# following: https://stackoverflow.com/questions/73110443/convert-a-tensorflow-script-to-pytorch-transformeddistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Electrodynamics with Geometric Algebra (WIP)\n",
    "\n",
    "## Theory overview\n",
    "Quantum Electrodynamics (QED) describes electrons, positrons (anti-electrons) and photons in a 4-dimensional spacetime with fields defined for all spacetime positions $X$. The 4-dimensional spacetime can be described by the [Spacetime Algebra (STA)](https://en.wikipedia.org/wiki/Spacetime_algebra) with basis vectors $\\gamma_0, \\gamma_1, \\gamma_2, \\gamma_3$ and corresponding metric $[1, -1, -1, -1]$. It contains two fields. The electron-positron field is a bispinor-field $\\psi(X)$ which in the context of Geometric Algebra (GA) is described by even-grade multivectors of the STA. The photon field $A(X)$ is a vector-field (ie. multivectors of degree 1, one basis for each dimension).\n",
    "\n",
    "A field configuration, also known as a path, $P(X)$ contains values for the two fields at every spacetime position. Our goal is to calculate the QED action using GA which allows us to use algorithms that solve for field configurations . The action is the negative log-likelihood (NLL) of the field configuration, meaning it is a number which tells how likely a given field configuration is. It is not a probability as it is unnormalized. However even with only the NLL we can use sampling algorithms (eg. [Markov-Chain Monte-Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo), [Variational Inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods)) to sample field configurations so that the sampled distribution matches the normalized distribution.\n",
    "\n",
    "The Lagrangian is given in Hestenes' article [Real Dirac Theory](https://www.researchgate.net/publication/266578033_REAL_DIRAC_THEORY) in equation (B.6) as\n",
    "\n",
    "$\\mathcal{L} = \\langle \\hbar (\\nabla \\psi(X)) i \\gamma_3 \\widetilde{\\psi}(X) - e A(X) \\psi(X) \\gamma_0 \\widetilde{\\psi}(X) - m \\psi(X) \\widetilde{\\psi}(X) \\rangle$\n",
    "\n",
    "where $\\langle ... \\rangle$ denotes getting the scalar part, $i = \\gamma_2 \\gamma_1$, $\\nabla = \\sum_{i=0}^{3} \\gamma_i \\delta^i$ and $\\widetilde{\\psi}(X)$ is the grade-reversal of $\\psi$.\n",
    "\n",
    "The action $S(P)$ for a field-configuration $P=(\\psi, A)$ is calculated by integrating the Lagrangian $\\mathcal{L}(P, X)$ over all space-time positions $X$.\n",
    "\n",
    "$S(\\psi, A) = \\int_{X \\in \\mathcal{X}} \\mathcal{L}(\\psi, A, X) dx$\n",
    "\n",
    "Finally as we are doing this numerically we need to discretize spacetime into a 4-dimensional grid. Integrals over spacetime then become sums over the grid. Derivatives become finite-differences or more complicated operations to avoid the [aliasing](https://arxiv.org/abs/hep-lat/0207008) which results in the [fermion doubling](https://en.wikipedia.org/wiki/Fermion_doubling) problem.\n",
    "\n",
    "## Getting started\n",
    "Let's start by defining the spacetime algebra as a geometric algebra in 1 time and 3 space dimensions with metric $[1, -1, -1, -1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta = GeometricAlgebra([1, -1, -1, -1])\n",
    "for basis in sta.basis_mvs:\n",
    "    sta.print(basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our four basis vectors displayed here each with a different ... basis. Let's try squaring them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"e_0^2:\", sta(sta.e0) ** 2)\n",
    "print(\"e_1^2:\", sta(sta.e1) ** 2)\n",
    "print(\"e_2^2:\", sta(sta.e2) ** 2)\n",
    "print(\"e_3^2:\", sta(sta.e3) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squaring the basis vectors gave us back another purely scalar multivector. The squared bases indeed return the correct metric.\n",
    "\n",
    "We can create new multivectors of different kinds using the geometric algebra `sta_ga` object. Let's create some vectors such as the elements of the photon field and perform some operations to get a feel for them. We can use the methods on `sta_ga`, most of which take a `batch_shape` that says how many elements you want (`[]` meaning just a single one) and a `kind` that describes which elements it will set (eg. `\"even\"`, `\"mv\"` (meaning all), `\"vector\"`, `\"scalar\"`, ...). Alternatively we can just build everything out of the basis vectors ourselves by adding and multiplying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = sta.from_tensor_with_kind(torch.ones(4), kind=\"vector\")\n",
    "sta.print(\"v1:\", v1)\n",
    "\n",
    "v2 = sta.basis_mvs[0] + sta.basis_mvs[1]\n",
    "sta.print(\"v2:\", v2)\n",
    "\n",
    "sta.print(\"v1 * v2 (Geometric product):\", sta.geom_prod(v1, v2))\n",
    "sta.print(\"v1 | v2 (Inner product):\", sta.inner_prod(v1, v2))\n",
    "sta.print(\"v1 ^ v2 (Exterior product):\", sta.ext_prod(v1, v2))\n",
    "\n",
    "v3 = v1 + v2\n",
    "sta.print(\"v3 = v1 + v2:\", v3)\n",
    "sta.print(\"v1 | v3:\", sta.inner_prod(v1, v3))\n",
    "sta.print(\"v1 ^ v3:\", sta.ext_prod(v1, v3))\n",
    "\n",
    "v4 = sta.geom_prod(v1, v2)\n",
    "sta.print(\"v4 = v1 * v2:\", v3)\n",
    "sta.print(\"v1^-1 * v4:\", sta.geom_prod(sta.inverse(v1), v4), \"should be\", v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: MultiVector[1.00*e_0 + 1.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# v2: MultiVector[1.00*e_0 + 1.00*e_1]\n",
    "# v1 * v2 (Geometric product): MultiVector[-1.00*e_02 + -1.00*e_03 + -1.00*e_12 + -1.00*e_13]\n",
    "# v1 | v2 (Inner product): MultiVector[]\n",
    "# v1 ^ v2 (Exterior product): MultiVector[-1.00*e_02 + -1.00*e_03 + -1.00*e_12 + -1.00*e_13]\n",
    "# v3 = v1 + v2: MultiVector[2.00*e_0 + 2.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# v1 | v3: MultiVector[-2.00*1]\n",
    "# v1 ^ v3: MultiVector[-1.00*e_02 + -1.00*e_03 + -1.00*e_12 + -1.00*e_13]\n",
    "# v4 = v1 * v2: MultiVector[2.00*e_0 + 2.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# v1^-1 * v4: MultiVector[1.00*e_0 + 1.00*e_1] should be MultiVector[1.00*e_0 + 1.00*e_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for the bispinors (elements of even degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = sta.from_tensor_with_kind(torch.ones(8), kind=\"even\")\n",
    "sta.print(\"b1:\", b1)\n",
    "\n",
    "b2 = sta.from_scalar(4.0) + sta.geom_prod(sta.basis_mvs[0], sta.basis_mvs[1]) + sta.geom_prod(sta.basis_mvs[0], sta.basis_mvs[1])\n",
    "sta.print(\"b2:\", b2)\n",
    "\n",
    "sta.print(\"b1 | b2:\", sta.inner_prod(b1, b2))\n",
    "sta.print(\"b1 ^ b2:\", sta.ext_prod(b1, b2))\n",
    "\n",
    "b3 = sta.geom_prod(b1, b2)\n",
    "sta.print(\"b3 = b1 * b2:\", b3)\n",
    "sta.print(\"b3 * b2^-1:\", sta.geom_prod(b3, sta.inverse(b2)), \"should be\", b1)\n",
    "\n",
    "sta.print(\"~b2 (Grade reversal):\", sta.reversion(b2))\n",
    "sta.print(\"Scalar part of b2:\", sta.keep_blades_with_name(b2, \"\"))\n",
    "sta.print(\"e_01 part of b2:\", sta.keep_blades_with_name(b2, \"01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1: MultiVector[1.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123]\n",
    "# b2: MultiVector[4.00*1 + 2.00*e_01]\n",
    "# b1 | b2: MultiVector[6.00*1 + 6.00*e_01 + 4.00*e_02 + 4.00*e_03 + 4.00*e_12 + 4.00*e_13 + 6.00*e_23 + 4.00*e_0123]\n",
    "# b1 ^ b2: MultiVector[4.00*1 + 6.00*e_01 + 4.00*e_02 + 4.00*e_03 + 4.00*e_12 + 4.00*e_13 + 4.00*e_23 + 6.00*e_0123]\n",
    "# b3 = b1 * b2: MultiVector[6.00*1 + 6.00*e_01 + 6.00*e_02 + 6.00*e_03 + 6.00*e_12 + 6.00*e_13 + 6.00*e_23 + 6.00*e_0123]\n",
    "# b3 * b2^-1: MultiVector[1.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123] should be MultiVector[1.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123]\n",
    "# ~b2 (Grade reversal): MultiVector[4.00*1 + -2.00*e_01]\n",
    "# Scalar part of b2: MultiVector[4.00*1]\n",
    "# e_01 part of b2: MultiVector[2.00*e_01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we hopefully have some feel for how to operate with the geometric algebra numbers. So far we only worked with single numbers, but we can define a field (ie. a number for every grid point) by passing in a `batch_shape` that is the size of our grid. When printing the fields we won't see the actual numbers anymore, we will only see which blades are non-zero and the batch shape. However we can still access all of the numbers with the usual indexing rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "a = sta.from_tensor_with_kind(torch.ones((10, 10, 10, 10, 4)), kind=\"vector\")\n",
    "sta.print(\"A(X):\", a)\n",
    "\n",
    "sta.print(\"A(t=0, x=5, y=3, z=9):\", a[0, 5, 3, 9])\n",
    "sta.print(\"A(t=0, z=[3,4,5]):\", a[0, :, :, 3:6])\n",
    "sta.print(\"e_0 part of A(X):\", sta.select_blades_with_name(a, \"0\").shape)\n",
    "\n",
    "sta.print(\"A(0, 0, 0, 0) * ~A(0, 0, 0, 0):\", sta.geom_prod(a, sta.reversion(a))[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A(X): MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# A(t=0, x=5, y=3, z=9): MultiVector[1.00*e_0 + 1.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# A(t=0, z=[3,4,5]): MultiVector[batch_shape=(10, 10, 3)]\n",
    "# e_0 part of A(X): (10, 10, 10, 10)\n",
    "# A(0, 0, 0, 0) * ~A(0, 0, 0, 0): MultiVector[-2.00*1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you will probably believe me that we can do the same to create a bispinor field, so instead let's see how we can calculate derivatives.\n",
    "\n",
    "As mentioned in the beginning, derivatives become finite differences. To calculate finite differences we can take a copy of the field, shift it back by one in a dimension and subtract it. For instance of we were to calculate the derivative\n",
    "in the time direction we would shift the entire field by -1 along the time axis to get `A(X + TimeDirection * GridSpacing)` and subtract the actual field from this shifted field. All that is left then is to divide by the grid spacing.\n",
    "\n",
    "`d/dt A(X) = (A(X + TimeDirection * GridSpacing) - A(X)) / GridSpacing`\n",
    "\n",
    "To actually do the shifting we will use the `with_changes` method which allows copying of the multivector and overriding of its blade values so we will just shift the blade values themselves using [tf.roll](https://www.tensorflow.org/api_docs/python/tf/roll). A better abstraction that doesn't require using the internal blade values might be added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def finite_differences(field, axis, spacing):\n",
    "    shifted_field = torch.roll(field, shifts=-1, dims=axis)\n",
    "    return (shifted_field - field) / spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_t_a = finite_differences(a, axis=0, spacing=0.1)\n",
    "sta.print(\"d/dt A(X) = (A(X + TimeDirection * GridSpacing) - A(X)) / GridSpacing:\", deriv_t_a)\n",
    "sta.print(\"d/dt A(0, 0, 0, 0):\", deriv_t_a[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d/dt A(X) = (A(X + TimeDirection * GridSpacing) - A(X)) / GridSpacing: MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# d/dt A(0, 0, 0, 0): MultiVector[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe expectedly, as our field is just a constant value everywhere, we are left with a field that is zero everywhere. Now we have a finite differences operation that will work on fields of any kind.\n",
    "\n",
    "Now we have all the tools we need to actually calculate the QED action given a field configuration. As a reminder, the QED Lagrangian is given by\n",
    "\n",
    "$\\mathcal{L} = \\langle \\hbar (\\nabla \\psi(X)) i \\gamma_3 \\widetilde{\\psi}(X) - e A(X) \\psi(X) \\gamma_0 \\widetilde{\\psi}(X) - m \\psi(X) \\widetilde{\\psi}(X) \\rangle$\n",
    "\n",
    "and the action $S(\\psi, A)$ is the spacetime integral (now sum) over it.\n",
    "\n",
    "Let's start with the mass term on the right $m \\psi(X) \\widetilde{\\psi}(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def get_mass_term(psi, electron_mass):\n",
    "    return electron_mass * sta.geom_prod(psi, sta.reversion(psi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define psi as some arbitrary even-graded field for now\n",
    "psi = sta.from_tensor_with_kind(torch.ones([10, 10, 10, 10, 8]), kind=\"even\") + sta.from_tensor_with_kind(torch.ones([10, 10, 10, 10, 1]), kind=\"scalar\")\n",
    "sta.print(\"Psi:\", psi)\n",
    "sta.print(\"Psi at (0, 0, 0, 0):\", psi[0, 0, 0, 0])\n",
    "\n",
    "# The electron mass in planck units (hbar=1, c=1) is actually not 1 but something tiny.\n",
    "# However we won't bother with it for now.\n",
    "mass_term = get_mass_term(psi=psi, electron_mass=1.0)\n",
    "sta.print(\"Mass term:\", mass_term)\n",
    "sta.print(\"Mass term at (0, 0, 0, 0):\", mass_term[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psi: MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# Psi at (0, 0, 0, 0): MultiVector[2.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123]\n",
    "# Mass term: MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# Mass term at (0, 0, 0, 0): MultiVector[3.00*1 + 2.00*e_0123]\n",
    "\n",
    "# MultiVector[1.00*e_0]\n",
    "# MultiVector[1.00*e_1]\n",
    "# MultiVector[1.00*e_2]\n",
    "# MultiVector[1.00*e_3]\n",
    "# e_0^2: MultiVector[1.00*1]\n",
    "# e_1^2: MultiVector[-1.00*1]\n",
    "# e_2^2: MultiVector[-1.00*1]\n",
    "# e_3^2: MultiVector[-1.00*1]\n",
    "# v1: MultiVector[1.00*e_0 + 1.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# v2: MultiVector[1.00*e_0 + 1.00*e_1]\n",
    "# v1 * v2 (Geometric product): MultiVector[-1.00*e_02 + -1.00*e_03 + -1.00*e_12 + -1.00*e_13]\n",
    "# v1 | v2 (Inner product): MultiVector[]\n",
    "# v1 ^ v2 (Exterior product): MultiVector[-1.00*e_02 + -1.00*e_03 + -1.00*e_12 + -1.00*e_13]\n",
    "# v3 = v1 + v2: MultiVector[2.00*e_0 + 2.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# v1 | v3: MultiVector[-2.00*1]\n",
    "# v1 ^ v3: MultiVector[-1.00*e_02 + -1.00*e_03 + -1.00*e_12 + -1.00*e_13]\n",
    "# v4 = v1 * v2: MultiVector[2.00*e_0 + 2.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# v1^-1 * v4: MultiVector[1.00*e_0 + 1.00*e_1] should be MultiVector[1.00*e_0 + 1.00*e_1]\n",
    "# b1: MultiVector[1.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123]\n",
    "# b2: MultiVector[4.00*1 + 2.00*e_01]\n",
    "# b1 | b2: MultiVector[6.00*1 + 6.00*e_01 + 4.00*e_02 + 4.00*e_03 + 4.00*e_12 + 4.00*e_13 + 6.00*e_23 + 4.00*e_0123]\n",
    "# b1 ^ b2: MultiVector[4.00*1 + 6.00*e_01 + 4.00*e_02 + 4.00*e_03 + 4.00*e_12 + 4.00*e_13 + 4.00*e_23 + 6.00*e_0123]\n",
    "# b3 = b1 * b2: MultiVector[6.00*1 + 6.00*e_01 + 6.00*e_02 + 6.00*e_03 + 6.00*e_12 + 6.00*e_13 + 6.00*e_23 + 6.00*e_0123]\n",
    "# b3 * b2^-1: MultiVector[1.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123] should be MultiVector[1.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123]\n",
    "# ~b2 (Grade reversal): MultiVector[4.00*1 + -2.00*e_01]\n",
    "# Scalar part of b2: MultiVector[4.00*1]\n",
    "# e_01 part of b2: MultiVector[2.00*e_01]\n",
    "# A(X): MultiVector[batch_shape=torch.Size([10, 10, 10, 10])]\n",
    "# A(t=0, x=5, y=3, z=9): MultiVector[1.00*e_0 + 1.00*e_1 + 1.00*e_2 + 1.00*e_3]\n",
    "# A(t=0, z=[3,4,5]): MultiVector[batch_shape=torch.Size([10, 10, 3])]\n",
    "# e_0 part of A(X): torch.Size([10, 10, 10, 10])\n",
    "# A(0, 0, 0, 0) * ~A(0, 0, 0, 0): MultiVector[-2.00*1]\n",
    "# d/dt A(X) = (A(X + TimeDirection * GridSpacing) - A(X)) / GridSpacing: MultiVector[batch_shape=torch.Size([10, 10, 10, 10])]\n",
    "# d/dt A(0, 0, 0, 0): MultiVector[]\n",
    "# Psi: MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# Psi at (0, 0, 0, 0): MultiVector[2.00*1 + 1.00*e_01 + 1.00*e_02 + 1.00*e_03 + 1.00*e_12 + 1.00*e_13 + 1.00*e_23 + 1.00*e_0123]\n",
    "# Mass term: MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# Mass term at (0, 0, 0, 0): MultiVector[3.00*1 + 2.00*e_0123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the interaction term in the center that describes the scattering between the electron-positron field and the photon field $e A(X) \\psi(X) \\gamma_0 \\widetilde{\\psi}(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_term(psi, a, electron_charge):\n",
    "    return sta.geom_prod(electron_charge * a, sta.geom_prod(psi, sta.geom_prod(sta.e(\"0\"), sta.reversion(psi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_term = get_interaction_term(psi=psi, a=a, electron_charge=1.0)\n",
    "sta.print(\"Interaction term:\", interaction_term)\n",
    "sta.print(\"Interaction term at (0, 0, 0, 0):\", interaction_term[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction term: MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# Interaction term at (0, 0, 0, 0): MultiVector[25.00*1 + -13.00*e_01 + -13.00*e_02 + -21.00*e_03 + -8.00*e_13 + -8.00*e_23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the momentum term for which we needed the finite differences $\\hbar (\\nabla \\psi(X)) i \\gamma_3 \\widetilde{\\psi}(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def get_momentum_term(psi, spacing, hbar):\n",
    "    # Nabla Psi\n",
    "    dt_psi = finite_differences(psi, axis=0, spacing=spacing)\n",
    "    dx_psi = finite_differences(psi, axis=1, spacing=spacing)\n",
    "    dy_psi = finite_differences(psi, axis=2, spacing=spacing)\n",
    "    dz_psi = finite_differences(psi, axis=3, spacing=spacing)\n",
    "    d_psi = dt_psi + dx_psi + dy_psi + dz_psi\n",
    "\n",
    "    return sta.geom_prod(hbar * d_psi, sta.geom_prod(sta.e(\"213\"), sta.reversion(psi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_term = get_momentum_term(psi=psi, spacing=0.1, hbar=1.0)\n",
    "\n",
    "sta.print(\"Momentum term:\", momentum_term)\n",
    "sta.print(\"Momentum term at (0, 0, 0, 0):\", momentum_term[0, 0, 0, 0]) # Still zero ;("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum term: MultiVector[batch_shape=(10, 10, 10, 10)]\n",
    "# Momentum term at (0, 0, 0, 0): MultiVector[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the terms, we can add them up, sum over all grid points and take the scalar part to get the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(psi, a, spacing, electron_mass, electron_charge, hbar):\n",
    "    mass_term = get_mass_term(psi=psi, electron_mass=electron_mass)\n",
    "    interaction_term = get_interaction_term(psi=psi, a=a, electron_charge=electron_charge)\n",
    "    momentum_term = get_momentum_term(psi=psi, spacing=spacing, hbar=hbar)\n",
    "\n",
    "    # Sum terms and get scalar part\n",
    "    lagrangians = (momentum_term - mass_term - interaction_term)[..., 0]\n",
    "\n",
    "    # Sum lagrangians (one lagrangian for each spacetime point) over spacetime\n",
    "    # to get a single value, the action.\n",
    "    return torch.sum(lagrangians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = get_action(psi=psi, a=a, spacing=0.1, electron_mass=1.0, electron_charge=1.0, hbar=1.0)\n",
    "print(\"Action:\", action)\n",
    "\n",
    "# Action: tf.Tensor(-280000.0, shape=(), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cython\n",
    "# !pip install pyhmc\n",
    "# !pip install statsmodels\n",
    "# !pip install triangle\n",
    "# !pip install corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can calculate the action for a given field configuration (ie. values for `psi` and `a` at every grid point) we could use a sampling algorithm\n",
    "to sample fields and calculate quantities of interest such as the correlation function, vacuum energy and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(psi_config, a_config):\n",
    "    mv_psi_config = sta.from_tensor_with_kind(psi_config, \"even\")\n",
    "    mv_a_config = sta.from_tensor_with_kind(a_config, \"vector\")\n",
    "\n",
    "    action = get_action(mv_psi_config, mv_a_config, spacing=0.0000001, electron_mass=0.00001,electron_charge=0.0854245, hbar=1.0)\n",
    "\n",
    "    # Action is the negative log likelihood of the fields, and since\n",
    "    # the sampling function expects a (positive) log likelihood,\n",
    "    # we return the negation.\n",
    "    return -action\n",
    "\n",
    "num_chains = 50\n",
    "\n",
    "\n",
    "# https://adamcobb.github.io/journal/hamiltorch.html\n",
    "\n",
    "\"\"\"\n",
    "kernel=tfp.mcmc.NoUTurnSampler(\n",
    "    target_log_prob_fn=joint_log_prob,\n",
    "    step_size=step_size\n",
    "),\n",
    "kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=joint_log_prob,\n",
    "            step_size=step_size,\n",
    "            num_leapfrog_steps=3\n",
    "        ),\n",
    "\"\"\"\n",
    "\n",
    "# @tf.function(experimental_compile=False)\n",
    "# def sample(initial_state, step_size):\n",
    "#     return tfp.mcmc.sample_chain(\n",
    "#         num_results=300,\n",
    "#         num_burnin_steps=1000,\n",
    "#         current_state=initial_state,\n",
    "#         kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "#             target_log_prob_fn=joint_log_prob,\n",
    "#             step_size=step_size,\n",
    "#             num_leapfrog_steps=3\n",
    "#         ),\n",
    "#         trace_fn=None\n",
    "#     )\n",
    "\n",
    "\n",
    "# https://github.com/rmcgibbo/pyhmc\n",
    "# !pip install cython\n",
    "# !pip install pyhmc\n",
    "# !pip install statsmodels\n",
    "# !pip install triangle\n",
    "\n",
    "import numpy as np\n",
    "def logprob(x, ivar):\n",
    "    logp = -0.5 * np.sum(ivar * x**2)\n",
    "    grad = -ivar * x\n",
    "    return logp, grad\n",
    "\n",
    "from pyhmc import hmc\n",
    "ivar = 1. / np.random.rand(5)\n",
    "samples = hmc(logprob, x0=np.random.randn(5), args=(ivar,), n_samples=int(1e4))\n",
    "\n",
    "# Using the beautiful $ pip install triangle_plot\n",
    "# import triangle\n",
    "\n",
    "# import corner\n",
    "# figure = corner.corner(samples)\n",
    "\n",
    "# figure.savefig('triangle.png')\n",
    "# figure.plot()\n",
    "\n",
    "# def sample(initial_state, step_size):\n",
    "#     return tfp.mcmc.sample_chain(\n",
    "#         num_results=300,\n",
    "#         num_burnin_steps=1000,\n",
    "#         current_state=initial_state,\n",
    "#         kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "#             target_log_prob_fn=joint_log_prob,\n",
    "#             step_size=step_size,\n",
    "#             num_leapfrog_steps=3\n",
    "#         ),\n",
    "#         trace_fn=None\n",
    "#     )\n",
    "\n",
    "\n",
    "gs = 6 # grid size\n",
    "initial_state = [\n",
    "    # Psi (bispinor field, 8 components)\n",
    "    # A (vector field, 4 components)\n",
    "    torch.zeros((num_chains, gs, gs, gs, gs, 8), dtype=tf.float32),\n",
    "    torch.zeros((num_chains, gs, gs, gs, gs, 4), dtype=tf.float32)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "variable_step_size = [0.001, 0.001]\n",
    "\n",
    "chain_samples = hmc(logprob, x0=initial_state, args=(variable_step_size,), n_samples=int(300))\n",
    "\n",
    "# chain_samples = sample(initial_state, variable_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# (300, 50, 6, 6, 6, 6, 8)\n",
    "# (300, 50, 6, 6, 6, 6, 4)\n",
    "# tf.Tensor(25.011002, shape=(), dtype=float32)\n",
    "# tf.Tensor(24.947697, shape=(), dtype=float32)\n",
    "\n",
    "print(chain_samples[0].shape)\n",
    "print(chain_samples[1].shape)\n",
    "print(tf.reduce_sum(tf.abs(chain_samples[0][0, 0] - chain_samples[0][1, 0])))\n",
    "print(tf.reduce_sum(tf.abs(chain_samples[0][1, 0] - chain_samples[0][2, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(5, 5))\n",
    "for i in range(4):\n",
    "    ax = axes[i % 2][i // 2]\n",
    "    im = ax.imshow(chain_samples[1][0, 0, 0, 0, :, :, i])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i % 2][i // 2]\n",
    "    im = ax.imshow(chain_samples[0][0, 0, 0, 0, :, :, i])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i % 2][i // 2]\n",
    "    im = ax.imshow(chain_samples[0][0, 0, 0, 1, :, :, i])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i % 2][i // 2]\n",
    "    im = ax.imshow(chain_samples[0][0, 0, 0, 2, :, :, i])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i % 2][i // 2]\n",
    "    im = ax.imshow(chain_samples[0][0, 0, 0, 0, :, :, i])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i % 2][i // 2]\n",
    "    im = ax.imshow(chain_samples[0][100, 0, 0, 0, :, :, i])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "fig.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i % 2][i // 2]\n",
    "    im = ax.imshow(chain_samples[0][200, 0, 0, 0, :, :, i])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(\"bmh\"):\n",
    "    def plot_correlations(ax, samples, axis):\n",
    "        correlation_by_shift = []\n",
    "        correlation_std_by_shift = []\n",
    "        shifts = list(range(1, samples.shape[axis]))\n",
    "\n",
    "        #if samples.shape[-1] == 8:\n",
    "        #    samples = sta.from_tensor_with_kind(samples, \"even\")\n",
    "        #elif samples.shape[-1] == 4:\n",
    "        #    samples = sta.from_tensor_with_kind(samples, \"vector\")\n",
    "\n",
    "        for i in shifts:\n",
    "            shifted = tf.roll(samples, shift=-i, axis=axis)\n",
    "            correlations = tf.reduce_mean(samples * shifted, axis=[-1, -2, -3, -4, -5])\n",
    "            #correlations = tf.reduce_mean(sta.inner_prod(samples, shifted), axis=[-1, -2, -3, -4, -5])\n",
    "            correlation_by_shift.append(tf.reduce_mean(correlations))\n",
    "            correlation_std_by_shift.append(tf.math.reduce_std(correlations))\n",
    "        ax.errorbar(shifts, correlation_by_shift, correlation_std_by_shift, capsize=5)\n",
    "\n",
    "    fig, axes = plt.subplots(4, sharex=True, sharey=True, figsize=(14, 8))\n",
    "    plot_correlations(axes[0], chain_samples[0], axis=-2)\n",
    "    plot_correlations(axes[1], chain_samples[0], axis=-3)\n",
    "    plot_correlations(axes[2], chain_samples[0], axis=-4)\n",
    "    plot_correlations(axes[3], chain_samples[0], axis=-5)\n",
    "\n",
    "    fig, axes = plt.subplots(4, sharex=True, sharey=True, figsize=(14, 8))\n",
    "    plot_correlations(axes[0], chain_samples[1], axis=-2)\n",
    "    plot_correlations(axes[1], chain_samples[1], axis=-3)\n",
    "    plot_correlations(axes[2], chain_samples[1], axis=-4)\n",
    "    plot_correlations(axes[3], chain_samples[1], axis=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "10ff9cff1e0293119a5a1fdb939fd95de09217e1f26359b091d157f526f5e737"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
